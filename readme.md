# **Public Service Commission RAG Chatbot**

## **ğŸ“Œ Project Overview**

This project implements a **Retrieval-Augmented Generation (RAG) pipeline** using **Weaviate** as a vector database, **Cohere embeddings**, and **FastAPI** for backend processing. The frontend interface is built with **Streamlit**, allowing users to search **YouTube transcripts** from **Public Service Commission (PSC) meetings** and retrieve relevant insights.

## **ğŸ“‚ Project Structure**

```
Entergy-AI/
â”œâ”€â”€ RAG/                      # Retrieval Augmented Generation system
â”‚   â”œâ”€â”€ streamlit/            # Streamlit frontend
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ app.py            # Streamlit frontend for querying transcripts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ batch_upload.ipynb    # Notebook for batch uploading transcripts to Weaviate
â”‚   â”œâ”€â”€ load_transcripts.py   # Script for loading transcript data
â”‚   â”œâ”€â”€ routes.py             # FastAPI routes for the backend
â”‚   â”œâ”€â”€ upload_transcripts.ipynb  # Notebook for transcript upload workflows
â”‚   â”œâ”€â”€ weaviate_class.py     # Weaviate client wrapper class
â”‚   â”œâ”€â”€ weviate.py            # Weaviate utilities            
â”œâ”€â”€ .gitignore                # Git ignore file
â”œâ”€â”€ readme.md                 # Project documentation
â””â”€â”€ requirements.txt          # Dependencies for the project
```

---

## **âš™ï¸ Setup Instructions**

### **1ï¸âƒ£ Clone the Repository**

```bash
git clone https://github.com/[yourusername]/Entergy-AI.git
cd Entergy-AI/RAG
```

### **2ï¸âƒ£ Create & Activate Virtual Environment**

```bash
python3 -m venv psc_env
source psc_env/bin/activate  # macOS/Linux
psc_env\Scripts\activate    # Windows
```

### **3ï¸âƒ£ Install Dependencies**

```bash
pip install -r requirements.txt
```

### **4ï¸âƒ£ Set Up Environment Variables**

Create a `.env` file in the root directory and add your API keys:

```
WEAVIATE_URL="https://your-weaviate-instance.weaviate.cloud"
WEAVIATE_KEY="your_weaviate_api_key"
HUGGINGFACE_API_KEY="your_huggingface_api_key"
OPENAI_KEY="your_openai_api_key"
COHERE_KEY="your_cohere_api_key"
```

### **5ï¸âƒ£ Run Weaviate Schema Initialization**

Before uploading transcripts, ensure the schema exists in Weaviate:

```python
python rag_demo.py --init-schema ## WORK IN PROGRESS
```

---

## **ğŸš€ Running the Application**

### **1ï¸âƒ£ Start FastAPI Backend**

```bash
uvicorn fastapi_app:app --reload
```

âœ… FastAPI will be running at: **`http://127.0.0.1:8000`**

#### **Available API Endpoints:**

| Method | Endpoint  | Description            |
| ------ | --------- | ---------------------- |
| `POST` | `/ask`    | Query the RAG pipeline |
| `GET`  | `/health` | Check service health   |

### **2ï¸âƒ£ Start Streamlit Frontend**

```bash
streamlit run streamlit/app.py
```

âœ… Open **`http://localhost:8501`** in your browser.

---

## **ğŸ“¥ Uploading Transcripts to Weaviate**

To upload **Louisiana PSC transcripts**, run:

```python
python rag_demo.py --upload "data/Louisiana_Transcript_2024.json" ## WORK IN PROGRESS
```

---

## **ğŸ” Querying Transcripts**

### **Using API**

```bash
curl -X 'POST' \
  'http://127.0.0.1:8000/ask' \
  -H 'Content-Type: application/json' \
  -d '{"question": "What were the discussions on rate increases?"}'
```

### **Using Streamlit Interface**

Simply enter your query in the UI and view relevant transcript excerpts

---

## **ğŸ“Œ Features & Capabilities**

âœ… **Weaviate Integration** - Stores & retrieves transcripts efficiently  
âœ… **Hugging Face Vectorizer** - Generates embeddings for transcript search  
âœ… **FastAPI Backend** - Handles RAG pipeline & queries  
âœ… **Streamlit Frontend** - User-friendly search interface  
âœ… **Batch Processing** - Uploads & indexes transcripts in chunks  
âœ… **State-Based Search** - Filter results by transcript source (Louisiana, etc.)

---

## **ğŸ› ï¸ Future Enhancements**

- [ ] Add support for **multiple states** & filtering
- [ ] Implement **Hybrid Search** (BM25 + Vector Search)
- [ ] Improve **document chunking strategy** for better retrieval

---
